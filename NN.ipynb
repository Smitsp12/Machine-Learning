{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e91c921-3bac-4715-a7b8-12ecff8064b5",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33a1a4-e44d-4be9-8f28-f60cc403b7af",
   "metadata": {},
   "source": [
    "#simple forward propogation\n",
    "x = np.array([[200, 17]])\n",
    "y = np.array([0,1])\n",
    "layer_1 = Dense(units=3, activation='sigmoid')\n",
    "a1 = layer_1(x)\n",
    "\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "a2 = layer_2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ecb171-2418-42d2-9e26-ef0b15caf8e3",
   "metadata": {},
   "source": [
    "#step 1: Define the model and activation function\n",
    "model = Sequential([Dense(units=25, activation='sigmoid'), Dense(units=15, activation='sigmoid'), Dense(units=1, activation='sigmoid')]) #this function sequentially join two layers\n",
    "\n",
    "#step 2: define loss function\n",
    "from tensorflow.keras.losses import BinaryCrossentropy #same as logistic loss function just diff name\n",
    "#use loss=MeanSquaredError() for regression problem\n",
    "model.compile(loss=BinaryCrossentropy())   \n",
    "\n",
    "#step 3:train the model on 100 iteration\n",
    "model.fit(x,y, epochs=100)  #train model on x and y data\n",
    "\n",
    "#its gradient descent for 100 iterations. tf performs backpropgation in this fit function. or we can use adam algorithm as well\n",
    "\n",
    "#step 4:check new value of x\n",
    "model.predict(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48478755-1635-4b36-b0b7-8ce61f32324d",
   "metadata": {},
   "source": [
    "ReLU or rectified linear unit g(z) = max(0,z) is used in real life for all inner layers in a NN. \n",
    "\n",
    "- relu is easier to compute than sigmoid.\n",
    "- relu function is flat only one side. this flatness makes gradient descent more computational. intutively using relu, GD is faster than sigmoid.\n",
    "- linear activation function g(z) = z should not be used for inner layers as it will result in just linear combination\n",
    "- activation = 'sigmoid', 'linear','relu'\n",
    "\n",
    "choosing activation function for output layer\n",
    "\n",
    "1) binary classification: sigmoid\n",
    "2) regression: linear activation (eg. stock price modeling) if y can be positive or negative\n",
    "3) predecting non negative values = reLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce27c86-8472-4dcb-bd91-67f52e257d20",
   "metadata": {},
   "source": [
    "with softmax output we have an array of probabilities which tells where does out input belong. the element with highest probability wins.\n",
    "\n",
    "model = Sequential([Dense(units=25, activation='sigmoid'), Dense(units=15, activation='sigmoid'), Dense(units=10, activation='softmax')])\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy() \n",
    "model.compile(loss= SparseCategoricalCrossentropy())\n",
    "\n",
    "this refers to clasifying y in categories. this fuction tells that output can take one value.\n",
    "\n",
    "model.fit(x,y,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebd77a-9cb3-474a-bda8-1d5d95bf5c1c",
   "metadata": {},
   "source": [
    "to reduce the propogation of round off error in code due to large numbers used. we do not declare loss as an explicit equation but incorporate it directly in the next equaation. \n",
    "\n",
    "model = Sequential([Dense(units=25, activation='sigmoid'), Dense(units=15, activation='sigmoid'), Dense(units=10, activation='linear')]\n",
    "\n",
    "model.compile(loss= SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(x,y, epochs = 100)\n",
    "logit = model(x)\n",
    "\n",
    "f_x = tf.nn.sigmoid(logit)\n",
    "\n",
    "logits is simply the input. this code reduce numerical roundoff error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f2659-bb2f-4e9a-9749-a718a2e25d0c",
   "metadata": {},
   "source": [
    "#using adam \n",
    "\n",
    "adam basically controls the learning rate.\n",
    "\n",
    "if the gradient remains same for a while it increases the learning rate. whereas if it detects that if the gradient wiggesl between negative aand positive value than alpha descreases.\n",
    "\n",
    "to implement this use\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-03), loss =tf.keras.losses.SparseCategorialCrossentropy(from_logits=True))\n",
    "\n",
    "model.fit(x,y,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4c6b5-1348-4864-adb5-727df80e21ed",
   "metadata": {},
   "source": [
    "the basic idea behind CNN is that any neuron in a layer will not take the entire input from the previous layer, but on;y a chunk of output from previous layer.  it changes the architecture. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
